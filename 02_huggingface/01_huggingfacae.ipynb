{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Hugging Face\n",
    "---\n",
    "## 01_01 Hugging Face란?\n",
    "* 코드로 구현되어 있는 모델을 모아놓은 곳으로 AI의 github라 불린다.\n",
    "* 허깅페이스는 최신 AI 기술을 더 간편하게 활용 할 수 있또록 제공되는 플랫폼이며, 핵심기술은 Transformers 라이브러리이다.\n",
    "\n",
    "### 01_01_01 Hugging Face 장점\n",
    "* 모델을 처음부터 개발하고 훈련할 필요가 없다.\n",
    "* 사용 편의성 (간단하고 사용자 친화적인 인터페이스가 제공되어 쉽게 이용가능)\n",
    "* 지속적으로 발전 중 (AI의 최신기술을 공유받고 쉽게 접근 가능)\n",
    "\n",
    "\n",
    "## 01_01 Transformers 라이브러리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_01 자연어처리\n",
    "* 컴퓨터가 인간의 언어를 이해, 생성, 조작 할 수 있도록 해주는 인공지능의 한 분야\n",
    "\n",
    "**자연어 처리의 주요 단계**\n",
    "\n",
    "* 토큰화: 텍스트를 의미 있는 단위(토큰)로 나누는 과정  \n",
    "    예시: \"나는 학교에 간다\" -> [\"나는\", \"학교에\", \"간다\"]<br>\n",
    "\n",
    "* 단어 집합 생성: 텍스트에서 고유한 단어들을 모아 사전을 만드는 과정  \n",
    "    예시: [\"나는\", \"학교에\", \"간다\"] -> {\"나는\", \"학교에\", \"간다\"}<br>\n",
    "\n",
    "* 정수 인코딩: 단어를 고유한 정수로 변환하여 컴퓨터가 처리할 수 있게 하는 과정  \n",
    "    예시: {\"나는\": 1, \"학교에\": 2, \"간다\": 3} -> [1, 2, 3]<br>\n",
    "\n",
    "* 패딩: 서로 다른 길이의 시퀀스를 동일한 길이로 맞추는 과정  \n",
    "    예시: [1, 2, 3] -> [1, 2, 3, 0, 0] (길이 5로 패딩)<br>\n",
    "\n",
    "* 벡터화: 텍스트 데이터를 숫자 벡터로 변환하여 기계학습 모델에 입력할 수 있게 하는 과정  \n",
    "    예시: \"나는\" -> [0.1, 0.2, 0.3, ..., 0.9] (100차원 벡터)  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "**형태소 토큰화**  \n",
    "<br>\n",
    "원시문 예시 :  \n",
    "\"길을 가다가 고양이를 봤어. 고양이에게 츄르를 주려했는데 고양이가 도망가버렸어\"  \n",
    "\n",
    "토큰화 예시 :  \n",
    "[\"길을\", \"가다가\", \"고양이를\", \"봤어\", \".\", \"고양이에게\", \"츄르를\", \"주려했는데\", \"고양이가\", \"도망가버렸어\"]  \n",
    "\n",
    "똑같은 고양이가 총 3번 등장한다, 이때 \"를\", \"에게\", \"가\" 가 붙어있어 기계는 각기 다른 단어로 인식 할 수 있다.  \n",
    "따라서 한국에서는 보편적으로 '형태소 분석기'로 토큰화를 진행한다.  \n",
    "\n",
    "형태소 토큰화 예시 : \n",
    "[\"길\", \"을\", \"가다가\", \"고양이\", \"를\", \"보\", \"았\", \"어\", \".\", \"고양이\", \"에게\", \"츄르\", \"를\", \"주\", \"려\", \"하\", \"았\", \"는데\", \"고양이\", \"가\", \"도망가\", \"버리\", \"었\", \"어\"]  \n",
    "\n",
    "이렇게 형태소 단위로 분리하면 \"고양이\"라는 단어가 일관되게 인식되며, 조사나 어미는 별도로 분리된다.  \n",
    "이를 통해 기계가 더 정확하게 단어의 의미와 문법적 기능을 파악할 수 있다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리 발전 과정\n",
    "\n",
    "**카운트 기반 단어 표현**\n",
    "* 단어 표현 중 하나로, 단어의 빈도수를 기반으로 단어의 의미를 표현하는 방법\n",
    "* DTM(Document Term Matrix) : 문서 내 단어의 빈도수를 기반으로 문서를 표현하는 방법\n",
    "* TF-IDF : 단어의 중요도를 고려하여 문서를 표현하는 방법 <br>\n",
    "\n",
    "Bag of Words(단어주머니)\n",
    "* 문서 내 단어의 출현 여부를 기반으로 문서를 표현하는 방법\n",
    "* 단어의 출현 빈도로만 나타내기 때문에 단어의 순서는 고려하지 않는다.\n",
    "<br> => 문맥 파악이 불가능 (강아지가 놀고 있다 와 놀고 있는 강아지를 구별 못함)\n",
    "\n",
    "**단어임베딩(통계기반의 언어모델)**\n",
    "* 대표적인 알고리즘 : Word2vec, GloVe\n",
    "* 단어를 벡터로 표현하는 방법\n",
    "* 단어의 의미를 벡터로 표현하여 문맥을 고려할 수 있도록 해준다.\n",
    "<br> => 하지만 다의어 같은 (문맥에따라 다양한의미를 기지는걸 구별하지 못함)\n",
    "\n",
    "**딥러닝 기반 언어 모델**\n",
    "\n",
    "**시퀀스 모델**\n",
    "* 시퀀스란?\n",
    "<br>시퀀스는 순서가 있는 데이터의 집합을 의미한다.\n",
    "<br>자연어 처리에서 시퀀스는 주로 단어나 문자의 연속을 나타낸다.\n",
    "<br>예를 들어, 문장은 단어들의 시퀀스이고, 단어는 문자들의 시퀀스입니다.\n",
    "<br>시퀀스 데이터는 순서가 중요하며, 이전 요소가 다음 요소에 영향을 미칠 수 있다.<br>\n",
    "\n",
    "* RNN(Recurrent Neural Network)<br> \n",
    "순차적 데이터를 처리하는 신경망으로, 이전 단계의 정보를 현재 단계로 전달하는 구조를 가짐\n",
    "* LSTM(Long Short-Term Memory)<br>\n",
    "RNN의 장기 의존성 문제를 해결하기 위해 개발된 모델로, 장기 기억을 유지할 수 있는 구조를 가짐\n",
    "* GRU(Gated Recurrent Unit)<br>\n",
    "LSTM을 간소화한 모델로, 더 적은 매개변수를 사용하면서도 비슷한 성능을 보임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_02 Transformers 라이브러리란?\n",
    "* 허깅페이스에서 제공하는 최신 모델을 쉽게 사용할 수 있도록 도와주는 라이브러리이다.\n",
    "* 트랜스포머는 자연어 처리 분야에서 많이 사용되는 모델로, 다양한 언어 모델을 포함하고 있다.\n",
    "\n",
    "트랜스포머는 자연어 처리 분야의 주요 딥러닝 모델로, \"Attention is All You Need\" 논문에서 소개되었다.  \n",
    "Self-Attention 메커니즘을 사용하여 문맥 이해와 단어 간 관계 파악에 효과적이다.  \n",
    "기계 번역, 텍스트 생성, 질문 응답 등 다양한 작업에서 우수한 성능을 보인다.\n",
    "\n",
    "쉽게 생각하면 컴퓨터가 말하는 방법을 배우는 것과 비슷한 것으로 문장 속의 단어들이 어떤 관계를 가지고 있는지 파악하고 문장의 뜻을 이해하는데 기술이다\n",
    "\n",
    "* Transformers를 통해 사전학습된 최신 모델을 쉽게 다운로드하고 학습할 수 있는 API와 도구를 제공한다.\n",
    "\n",
    "**지원하는 기능**\n",
    "1) 자연어 처리 : 텍스트 분류, 엔티티 인식, 질문답변, 언어모델링, 요약, 번역, 객관식 텍스트 생성\n",
    "2) 컴퓨터 비전 : 이미지 분류, 물체감지 및 분할\n",
    "3) 오디오 : 음성인식 및 오디오 분류\n",
    "4) 멀티모달 : 테이블 질문 답변, 광학문자 인식, 스캔문서에서 정보추출, 비디오분류, 시각적 질문답변\n",
    "\n",
    "멀티모달이란 : 텍스트 이미지, 음성, 영상 등 다양한 양식으로 훈련해 다양한 결과물을 낼 수 있는 모델\n",
    "-> 즉, 여러가지 유형의 데이터를 동시에 다루는 기술\n",
    "\n",
    "https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#파이토치 설치\n",
    "#트랜스포머스 설치\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5206, 0.5525, 0.7532],\n",
      "        [0.3611, 0.2040, 0.8906],\n",
      "        [0.1515, 0.5446, 0.8259],\n",
      "        [0.6384, 0.8755, 0.5026],\n",
      "        [0.5392, 0.9284, 0.0318]])\n",
      "4.45.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_02 사용방법\n",
    "**pipeline()함수**<br>\n",
    "*다양한 task를 위한 인터페이스를 제공하여, 복잡한 모델을 쉽게 사용할수있게 해준다\n",
    "*특정모델과 동작에 필요한 전처리 및 후처리 단계를 연결하여 텍스트를 직접입력하고 이해하기 쉬운 답변을 얻을수있다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pipeline()함수의인자**<br>\n",
    "1. 공통 인자<br>\n",
    "task :수행할 작업의 종류를 지정\n",
    "model: 사용할 모델의 이름이나 경로를 지정\n",
    "tokenizer : 사용할 토크나이저 지정(모델과 함께 제공되는 토크나이져를 사용하는게 일반적)\n",
    "framework :사용할 딥러닝 프레임워크지정(pt:파이토치, tf:텐서플로우)\n",
    "device:실행할 장치 (cpu => -1, Gpu =>0), cuda\n",
    "<br>\n",
    "2. 작업별인자<br>\n",
    "* text-generation:\n",
    "* max_length: 생성할 텍스트의최대길이지정\n",
    "* num_return_sequence:생성할 텍스트의 개수 지정\n",
    "* temperature : 무작위성을 조절하는 파라미터\n",
    "<br>\n",
    "* question-answering:\n",
    "* context: 질문에 대한 답을 찾을 문맥제공\n",
    "* question :답을 찾고자하는 질문\n",
    "<br>\n",
    "* translation:\n",
    "* src_lang :원본 텍스트의언어지정\n",
    "* tgt_lang : 번역할 목표언어지정\n",
    "<br>\n",
    "* summarization:\n",
    "* min_length : 요약문에서 생성할 최소 단어수 지정\n",
    "* max_length : 요약문에서 생설할 최대 단어수 지정\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 22aad52 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# transformers 패키지에서 pipeline을 import\n",
    "from transformers import pipeline\n",
    "\n",
    "#음성을 인식해서 텍스트로 출력할수있는 모델\n",
    "#stt (automatic-speech-recognition) : 음성인식\n",
    "#pipeline에 작업을 인자로\n",
    "transcriber = pipeline('automatic-speech-recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#마틴루터킹 목사 연설줄 일부\n",
    "# i have a dream that one day this nation will rise up and live out the true meaning of its creed\n",
    "\n",
    "\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\80412\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v3-turbo\")\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoClass\n",
    "*  사전학습된 모델을 쉽게 사용할수있도록 도와주는 클래스\n",
    "* 수행하고자하는 task에 적합한 automodel과 autotokenizer를 선택해준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.AutoTokenizer\n",
    "* 텍스트를 모델이 처리할수있는 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#토큰화만 진행\n",
    "#google-bert/bert-base-uncased모델에 최적화된 토크나이저를 찾아내고\n",
    "#다운로드하여 사용할수있게 해준다\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Hello, my dog is cute\"\n",
    "\n",
    "#위의 문자를 토큰화해서 출력\n",
    "print(tokenizer(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. AutoImageProcessor\n",
    "* 이미지 데이터를 모델이 처리할수있는 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[ 0.1137,  0.1686,  0.1843,  ..., -0.1922, -0.1843, -0.1843],\n",
      "          [ 0.1373,  0.1686,  0.1843,  ..., -0.1922, -0.1922, -0.2078],\n",
      "          [ 0.1137,  0.1529,  0.1608,  ..., -0.2314, -0.2235, -0.2157],\n",
      "          ...,\n",
      "          [ 0.8353,  0.7882,  0.7333,  ...,  0.7020,  0.6471,  0.6157],\n",
      "          [ 0.8275,  0.7961,  0.7725,  ...,  0.5843,  0.4667,  0.3961],\n",
      "          [ 0.8196,  0.7569,  0.7569,  ...,  0.0745, -0.0510, -0.1922]],\n",
      "\n",
      "         [[-0.8039, -0.8118, -0.8118,  ..., -0.8902, -0.8902, -0.8980],\n",
      "          [-0.7882, -0.7882, -0.7882,  ..., -0.8745, -0.8745, -0.8824],\n",
      "          [-0.8118, -0.8039, -0.7882,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [-0.2706, -0.3176, -0.3647,  ..., -0.4275, -0.4588, -0.4824],\n",
      "          [-0.2706, -0.2941, -0.3412,  ..., -0.4824, -0.5451, -0.5765],\n",
      "          [-0.2784, -0.3412, -0.3490,  ..., -0.7333, -0.7804, -0.8353]],\n",
      "\n",
      "         [[-0.5451, -0.4667, -0.4824,  ..., -0.7412, -0.6941, -0.7176],\n",
      "          [-0.5529, -0.5137, -0.4902,  ..., -0.7412, -0.7098, -0.7412],\n",
      "          [-0.5216, -0.4824, -0.4667,  ..., -0.7490, -0.7490, -0.7647],\n",
      "          ...,\n",
      "          [ 0.5686,  0.5529,  0.4510,  ...,  0.4431,  0.3882,  0.3255],\n",
      "          [ 0.5451,  0.4902,  0.5137,  ...,  0.3020,  0.2078,  0.1294],\n",
      "          [ 0.5686,  0.5608,  0.5137,  ..., -0.2000, -0.4275, -0.5294]]]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "#google/vit-base-patch16-224모델을 사용하는데 최적화된 이미지 프로세서(전처리기)를 가져온다\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "#이미지 다운로드\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "#이미지 전처리\n",
    "inputs = image_processor(images=image, return_tensors='pt')\n",
    "\n",
    "print(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "#추론\n",
    "outputs = model(**inputs)\n",
    "\n",
    "#추론결과 출력\n",
    "logits = outputs.logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
